### Normalizition

- 什么是归一化/标准化 

它通过将数据进行偏移和尺度缩放调整，在数据预处理时是非常常见的操作，在网络的中间层如今也很频繁的被使用。 

- 种类

  - 线性归一化：$X'=(X-X_{min})/(X_{max}-X_{min})$
  - 零均值归一化/Z-score 标准化 :$X=(X-u)/b$   经过处理后的数据符合均值为 0，标准差为 1 的分布， 如果原始的分布是正态分布，那么 z-score 标准化就将原始的正态分布转换为标准正态分布 

  以上两种不会改变分布本身的形状

  - 正态分布 Box-Cox 变换 ：box-cox 变换可以将一个非正态分布转换为正态分布，使得分布具有对称性
  - 直方图均衡化 ：直方图均衡也可以将某一个分布归一化到另一个分布，它通过图像的灰度值分布，即图像直方图来对图像进行对比度进调整，可以增强局部的对比度。  

- 归一化的目标

归一化数据的目标，是为了让数据的分布变得更加符合期望，增强数据的表达能力

- Batch Normalization 

  - BN 的好处

    ​	(1) 减轻了对参数初始化的依赖，这是利于调参的朋友们的。
    ​	(2) 训练更快，可以使用更高的学习率。
    ​	(3) BN 一定程度上增加了泛化能力， dropout 等技术可以去掉。 

  - BN 的缺陷 ：batch normalization 依赖于 batch 的大小，当 batch 值很小时，计算的均值和方差不稳定。 不适用场景
    - batch 非常小，比如训练资源有限无法应用较大的 batch，也比如在线学习等使用单例进行模型参数更新的场景 
    - rnn，因为它是一个动态的网络结构，同一个 batch 中训练实例有长有短，导致每一个时间步长必须维持各自的统计量，这使得 BN 并不能正确的使用。 
  - 应用建议
    - 正常的处理图片的 CNN 模型都应该使用 Batch Normalization。只要保证 batch size 较大(不低于 32)，并且打乱了输入样本的顺序。如果 batch 太小，则优先用 GroupNormalization 替代。
    - 对于 RNN 等时序模型，有时候同一个 batch 内部的训练实例长度不一(不同长度的句子)，则不同的时态下需要保存不同的统计量，无法正确使用 BN 层，只能使用 Layer Normalization。
    - 对于图像生成以及风格迁移类应用，使用 Instance Normalization 更加合适。 

- Normalization 的有效性 
  - 主流观点， Batch Normalization 调整了数据的分布，不考虑激活函数，它让每一层的输出归一化到了均值为 0 方差为 1 的分布，这**保证了梯度的有效性**，目前大部分资料都这样解释，比如 BN 的原始论文认为的缓解了 Internal Covariate Shift(ICS)问题。
  - **可以使用更大的学习率**，文[2]指出 BN 有效是因为用上 BN 层之后可以**使用更大的学习率，从而跳出不好的局部极值，增强泛化能力**，在它们的研究中做了大量的实验来验证。
  - **损失平面平滑。**文[3]的研究提出， BN 有效的根本原因不在于调整了分布，因为即使是在 BN 层后模拟 ICS，也仍然可以取得好的结果。它们指出， BN 有效的根本原因是平滑了损失平面。之前我们说过， Z-score 标准化对于包括孤立点的分布可以进行更平滑的调整。 