---
layout: post
title: 机器学习基础
date: 2019-3-30
tag: 机器学习
---

[TOC]

### 1. 机器学习的四个分支

1. 监督学习
   1. 序列生成：给定图像，输出描述
   2. 语法树预测
   3. 目标检测：绘制边框
   4. 图像分割
2. 无监督学习
   1. 聚类
   2. 降维
3. 自监督学习
4. 强化学习：通过评价好坏来进行学习

### 2. 评估机器学习模型

#### 训练集、验证集、测试集

1. 简单留出验证
2. K 折验证
3. 数据打乱的重复 K 折验证

sklearn 模块有拆分数据集的功能

```python
# 拆分数据集作为训练集和验证集
from sklearn.model_selection import train_test_split
X_train,  X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size=0.1, random_state=2)
```

#### 评估模型的注意事项

1. 数据代表性：用来训练网络的数据应该尽量覆盖到整个样本的范围，通常做法是将数据随机打乱
2. 时间序列：对于预测未来的模型，时间是一个重要的属性，对于这类问题，数据不应该被打乱
3. 数据冗余：数据尽量不要有重复的样本，若重复的样本即出现在训练集又出现在测试集，容易造成数据泄漏，应该确保训练集和测试集数据之间没有交集

### 3. 数据预处理、特征工程、特征学习

#### 数据预处理

1. 向量化，将数据转换成神经网络可以处理的数据类型（张量），
   - **若是监督学习（分类）特别要注意数据和标签是一一对应的**
   - 若是分类，对应的编码技术

   ```python
   # keras 中的编码函数
   from keras.utils import to_categorical
   one_hot_train_labels = to_categorical(train_labels)
   ```

2. 值标准化（归一化）：若不同特征的范围差距非常大，会造成较大的梯度更新，导致模型无法收敛
   1. 取值较小（0-1）
   2. 同质性：不同特征值应该在大致相同的范围
   3. 特征标准化：是平均值为 0，标准差为 1。x -= x.mean; x /= x.std

3. 缺失值处理

4. 处理图像注意图像的格式 `{'png', 'jpg', 'jpeg', 'bmp', 'ppm'}`

#### 特征工程

特征工程的本质：用更简单的方法表述问题，使问题变得容易，需要深入理解问题的本质。可能的话加一点遐想。

结合具体的工程背景，提取所需特征。

特征工程的好处：

1. 良好的特征可以使用更少的资源解决问题
2. 良好的特征可以使用更少的数据解决问题

### 4. 过拟合与欠拟合

深度学习的模型很擅长拟合训练数据，但是真正的难度在于泛化！

过拟合判断：判断一个模型是否过拟合，让模型在训练数据上进行预测，若预测的精度很差，说明是模型没有训练成功，反之才是模型过拟合。

验证数据上的性能经过几轮迭代后达到最高点，然后开始下降——模型开始出现过拟合

#### 解决欠拟合

#### 降低过拟合方法

1. 获取更多的训练数据（最优）

2. 减小网络大小：在模型容量（网络参数数量）过大和模型容量不足取个折中

   1. 初始时选择较少的层和参数
   2. 依次增加层数或神经元数量，直至这种增加对**验证**损失的影响很小

3. 添加权重正则化（简单模型比复杂模型更不容易过拟合）：强制让模型权重只能取较小的值，从而限制模型的复杂度

   1. L1 正则化：添加的成本与权重系数的绝对值。模型的某些系数刚好为 0
   2. L2 正则化：添加的成本与权重系数的平方。模型的系数很小（接近于 0 ）但是不等于 0

4. 添加 dropout 正则化（dropout 是神经网络最有效也是最常用的方法之一）—— Geoffrey Hinton 开发

   1. 基本原理：在训练过程随机将该层的一些输出特征舍弃（设置为 0）

   2. dropout 比例：是被设置为 0 特征所占的比例，通常在 0.2-0.5 范围内。

   3. 测试时没有单元被舍弃，而该层的输出值需要按 dropout 比例缩小，因为这时比训练时有更多的单元被激活，需要加以平衡。（两种实现方式）

      1. 训练时使用 dropout 使某些参数为 0，之后在使输出的参数按 dropout 比例放大。（常用方式）
      2. 训练时使用 dropout 使某些参数为 0，测试时是输出按的 dropout 比例缩小。

      `keras 中有 dropout 层，可以方便的使用 dropout 正则化（重要的应该是考虑 dropout 比例？）`

5. early stopping。

6. 减少迭代次数。使用验证数据集的损失和精度曲线来帮助设置迭代次数

7. 增大学习率。

### 5. 机器学习通用流程

1. 定义问题，收集数据

   1. 输入数据是什么？预测什么？
   2. 要解决的是什么问题？（二分类、多分类、标量回归、向量回归、聚类、生成会强化学习）
   3. 做假设
      1. 假设输出是可以根据输入进行预测的
      2. 假设可用的数据包足够多的信息，足以从学习输出和输入之间的关系
      3. 其他问题：有规律性的变化对数据的要求

2. 选择衡量成功的指标（优化的目标）

   1. 平衡分类问题（每个类别的可能性相同）常用指标：精度和**接收者操作特征曲线线下面积**
   2. 类别不平衡问题：准确率和召回率
   3. 排序或多标签分类：平均准确率均值
   4. 自定义指标：Kaggle 网站的比赛不同问题的评估标准

   5. 确定评估方法
   6. 留出验证集：数据量大时使用
   7. K 折交叉验证：留给验证的样本量太少
      - K 折验证后，找到最合适的参数（轮数，网络层数，神经元数等）
      - 最后在模型上观察测试集的性能
   8. 重复 K 折验证：可用的数据很少

3. 数据准备与初始化

   1. 数据格式化为机器学习的格式（如张量）
   2. 归一化处理（取值进行缩放，不同特征取值缩放到一致的范围）
   3. 特征工程

   对于图像处理 keras  有图像处理辅助工具的模块  

   ```
   from keras.preprocessing.image import ImageDataGenerator
   ```

4. 开发比基准更好的模型

   1. 最后一层的激活：
   2. 损失函数：见下表
   3. 优化配置：优化器的选择？学习率的选择？`大多数情况下使用 rmsprop 及其默认的学习率是稳妥的`

   |                   问题类型                   | 最后一层激活 | 损失函数([loss](https://github.com/keras-team/keras/blob/master/keras/losses.py))——可以自定义 | 检测指标[metrics](https://keras.io/zh/metrics/) |
   | :------------------------------------------: | :----------: | :----------------------------------------------------------: | :---------------------------------------------: |
   |                    二分类                    |   sigmoid    |                     binary_crossentropy                      |                       acc                       |
   | 多分类、单标签~（一个样本点只能划分到一类）~ |   softmax    | categorical_crossentropy **one-hot 标签 衡量两个概率分布之间的距离**sparse_categorical_crossentropy ~（整数标签）~ |                       acc                       |
   |  多分类、多标签~（一个样本点能划分到多类）~  |   sigmoid    |                     binary_crossentropy                      |                                                 |
   |                 回归到任意值                 |      无      |                        mse(均方误差)                         |               mae (平均绝对误差)                |
   |            回归到 0-1 范围内的值             |   sigmoid    |                  mse 或 binary_crossentropy                  |                                                 |

5. 扩大模型规模：开发过拟合的模型（**越过过拟合再调节**）

   1. 添加更多的层
   2. 添加更多的神经元
   3. 训练更多的轮次

   观察模型在验证集上的性能

6. 模型正则化与调节超参数

   1. 添加 dropout
   2. 尝试不同的网络构架
   3. 正则化（L1、L2）
   4. 不同的超参数（每层的神经元个数、优化器的学习率）





