---
layout: post
title: 特征工程
date: 2019-10-19
tag: 机器学习
---

#####   特征归一化  

为了消除数据特征之间的量纲影响， 我们需要对特征进行归一化处理， 使得
不同指标之间具有可比性。  使各指标处于同一数值量级， 以便进行分析。  

-  知识点

特征归一化

- 问题

为什么需要对数值类型的特征做归一化

-   分析与解答  

对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值
区间内。最常用的方法主要有以下两种。  

1.   线性函数归一化（Min-Max Scaling）。 它对原始数据进行线性变换，使结果映射到 [0, 1] 的范围， 实现对原始数据的等比缩放。  
2.   零均值归一化（Z-Score Normalization）。它会将原始数据映射到均值为
   0、 标准差为 1 的分布上。  

 当然， 数据归一化并不是万能的。 在实际应用中， 通过梯度下降法求解的模
型通常是需要归一化的， 包括线性回归、 逻辑回归、 支持向量机、 神经网络等模
型。 但对于决策树模型则并不适用， 以 C4.5 为例， 决策树在进行节点分裂时主要
依据数据集 D 关于特征 x 的信息增益比（详见<u>百面机器学习算法</u>第 3 章第 3 节） ， 而信息增益比跟特征
是否经过归一化是无关的， 因为归一化并不会改变样本在特征 x 上的信息增益。  

##### 类别型特征  

  类别型特征（Categorical Feature） 主要是指性别（男、 女） 、 血型（A、 B、
AB、 O） 等只在有限选项内取值的特征。 类别型特征原始输入通常是字符串形
式， 除了决策树等少数模型能直接处理字符串形式的输入， 对于逻辑回归、 支持
向量机等模型来说， 类别型特征必须经过处理转换成数值型特征才能正确工作。  

- 知识点：

  序号编码（Ordinal Encoding） 、 独热编码（One-hot Encoding） 、 二进制编码
（Binary Encoding）  

- 问题

在对数据进行预处理时， 应该怎样处理类别型特征？  

- 分析与解答

1. 序号编码：序号编码通常用于处理**类别间具有大小关系**的数据。 例如成绩， 可以分为
   低、 中、 高三档， 并且存在“高>中>低”的排序关系。  
2. 独热编码： 独热编码通常用于**处理类别间不具有大小关系的特征**。 例如血型， 一共有4个
   取值（A型血、 B型血、 AB型血、 O型血） ， 独热编码会把血型变成一个4维稀疏
   向量， A型血表示为（1, 0, 0, 0） ， B型血表示为（0, 1, 0, 0） ， AB型表示为（0, 0,
   1, 0） ， O型血表示为（0, 0, 0, 1） 。   

需要注意的问题 ：使用稀疏向量来节省空间。  配合特征选择来降低维度。  

​		3. 二进制编码

##### 图像数据不足时的处理方法  

当训练一个图像分类模型时， 如果训练样本
比较少， 该如何处理呢？  

- 知识点：

迁移学习（Transfer Learning） ， 生成对抗网络， 图像处理， 上采样技术， 数据扩充  

- 问题

在图像分类任务中， 训练数据不足会带来什么问题？ 如何缓解数据量不足带来的问题？  

-   分析与解答 

  一个模型所能提供的信息一般来源于两个方面，一是**训练数据**中蕴含的信
息；二是在**模型的形成过程**中（包括构造、 学习、 推理等）， 人们提供的先验信息。当训练数据不足时，说明模型从原始数据中获取的信息比较少， 这种情况下
要想保证模型的效果， 就需要更多先验信息。 先验信息可以作用在模型上， 例如
让模型采用特定的内在结构、 条件假设或添加其他一些约束条件； 先验信息也可
以直接施加在数据集上， 即根据特定的先验假设去调整、变换或扩展训练数据，让其展现出更多的、更有用的信息，以利于后续模型的训练和学习。

具体到图像分类任务上， 训练数据不足带来的问题主要表现在过拟合方面，即模型在训练样本上的效果可能不错， 但在测试集上的泛化效果不佳。 根据上述
讨论， 对应的处理方法大致也可以分两类， 

一是基于模型的方法， 主要是采用降
低过拟合风险的措施， 包括简化模型（如将非线性模型简化为线性模型） 、 添加
约束项以缩小假设空间（如L1/L2正则项） 、 集成学习、 Dropout超参数等； 

二是基
于数据的方法， 主要通过数据扩充（Data Augmentation） ， 即根据一些先验知
识， 在保持特定信息的前提下， 对原始数据进行适当变换以达到扩充数据集的效果。 具体到图像分类任务中， 在保持图像类别不变的前提下， 可以对训练集中的
每幅图像进行以下变换。

- （1）一定程度内的随机旋转、 平移、 缩放、 裁剪、 填充、 左右翻转等， 这些
  变换对应着同一个目标在不同角度的观察结果。
- （2）对图像中的像素添加噪声扰动， 比如椒盐噪声、 高斯白噪声等。
- （3）颜色变换。 例如， 在图像的 RGB 颜色空间上进行主成分分析， 得到 3 个
  主成分的特征向量$p_1, p_2, p_3$及其对应的特征值 $λ_1,λ_2,λ_3$， 然后在每个像素的 RGB 值上
  添加增量$[p_1,p_2,p_3]•[α_1λ_1,α_2λ_2,α_3λ_3]^T$， 其中 $α_1,α_2,α_3$ 是均值为 0、 方差较小的高斯分布随
  机数。
- （4）改变图像的亮度、 清晰度、 对比度、 锐度等。  

除了直接在图像空间进行变换， 还可以先对图像进行特征提取，然后在图像
的特征空间内进行变换， 利用一些通用的数据扩充或上采样技术， 例如
SMOTE（Synthetic Minority Over-sampling Technique） 算法。 抛开上述这些启发式
的变换方法， 使用生成模型也可以合成一些新样本， 例如当今非常流行的生成式
对抗网络模型。 

类别不平衡问题：https://towardsdatascience.com/upgrade-your-image-classifier-with-balanced-data-ddea93859c0f 方案（暂时看到针对灰度图的）：

- 原始数据做 tSNE  可视化分析
- 数据经过 PCA（ https://scikit-learn.org/stable/modules/decomposition.html#principal-component-analysis-pca ） 降维以后做可视化分析
- 数据经过 PCA 降维以后，通过  SMOTE  解决类别不平衡的问题（生成更多的数据用于下一步进行分类），再做 tSNE 可视化分析
- 利用分类器做分类

此外， 借助已有的其他模型或数据来进行迁移学习在深度学习中也十分常
见。 例如， 对于大部分图像分类任务， 并不需要从头开始训练模型， 而是借用一
个在大规模数据集上预训练好的通用模型， 并在针对目标任务的小数据集上进行
微调（fine-tune） ， 这种微调操作就可以看成是一种简单的迁移学习。  