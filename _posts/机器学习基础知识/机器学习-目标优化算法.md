[TOC]
## 优化算法

优化算法对于深度学习⼗分重要。优化算法的表现直接影响模型的训练效率；另⼀⽅⾯，理解各种优化算法的原理以及其中超参数的意义将有助于我们更有针对性地调参，从而使深度学习模型表现更好。 

深度学习中，我们的优化的对象是损失函数，但是往往在优化的时候，我们并不能找到全局最优解，所以需要一些优化的算法来帮助我们实现最小化目标函数。

- 由于优化算法的⽬标函数通常是⼀个基于训练数据集的损失函数，优化的⽬标在于降低训练误差。
- 由于深度学习模型参数通常都是⾼维的，⽬标函数的鞍点通常⽐局部最小值更常⻅。

### 梯度下降和随机梯度下降

- 使⽤适当的学习率，沿着梯度反⽅向更新⾃变量可能降低⽬标函数值。梯度下降重复这⼀更新过程直到得到满⾜要求的解。
- 学习率过⼤或过小都有问题。⼀个合适的学习率通常是需要通过多次实验找到的。
- 当训练数据集的样本较多时，梯度下降每次迭代的计算开销较⼤，因而随机梯度下降通常更受⻘睐。 
- 随机梯度下降(stochastic gradient descent ) 每次从样本中随机抽取等均匀采样，然后计算梯度
- 梯度下降每次计算的复杂度为 O(n)
- 随机梯度是梯度的无偏估计

### ⼩批量随机梯度下降 

- 小批量随机梯度每次随机均匀采样⼀个小批量的训练样本来计算梯度。
-  在实际中，（小批量）随机梯度下降的学习率可以在迭代过程中⾃我衰减。
- 通常，小批量随机梯度在每个迭代周期的耗时介于梯度下降和随机梯度下降的耗时之间。
- 当批量较小时，每次迭代中使⽤的样本少，这会导致并⾏处理和内存使⽤效率变低。这使得在计算同样数⽬样本的情况下⽐使⽤更⼤批量时所花时间更多。当批量较⼤时，每个小批量梯度⾥可能含有更多的冗余信息。为了得到较好的解，**批量较⼤时⽐批量较小时需要计算的样本数⽬可能更多，例如增⼤迭代周期数。** 

### 动量法 

**为了解决⽬标函数在不同方向上梯度变化有明显差异的情况**，如下图：⽬标函数在竖直⽅向（x2轴⽅向）⽐在⽔平⽅向（x1轴⽅向）的斜率的绝对值更⼤。 因此，给定学习率，梯度下降迭代⾃变量时会使⾃变量在竖直⽅向⽐在⽔平⽅向移动幅度更⼤。那么，我们需要⼀个**较小的学习率**从而避免⾃变量在竖直⽅向上<u>（斜率绝对值大）</u>越过⽬标函数最优解。然而，这会造成⾃变量在⽔平⽅向上<u>（斜率绝对值小）</u>朝最优解移动变慢。 

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g4w6nepeumj20cf095q3i.jpg)

要是学习率再增大，则斜率大的那个方向有可能会发散，如下图

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g4w6tie2srj20dh09at91.jpg)

故引入动量法
由于小批量随机梯度下降比梯度下降更为广义，时间步 $t$ 的小批量随机梯度$\boldsymbol{g}_t$的定义。设时间步$t$的自变量为$\boldsymbol{x}_t$，学习率为$\eta_t$。在时间步 $0$，动量法创建速度变量 $\boldsymbol{v}_0$，并将其元素初始化成 0  。在时间步 $t>0$，动量法对每次迭代的步骤做如下修改：
$$
\begin{aligned}
\boldsymbol{v}_t &\leftarrow \gamma \boldsymbol{v}_{t-1} + \eta_t \boldsymbol{g}_t, \\
\boldsymbol{x}_t &\leftarrow \boldsymbol{x}_{t-1} - \boldsymbol{v}_t,
\end{aligned}
$$

其中，动量超参数$\gamma$满足$0 \leq \gamma < 1$。当$\gamma=0$时，动量法等价于小批量随机梯度下降。

- 动量法使⽤了指数加权移动平均的思想。它将过去时间步的梯度做了加权平均，且权重按时间步指数衰减。
  
- 动量法使得相邻时间步的⾃变量更新在⽅向上更加⼀致。
  
- 在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致。 
  


### AdaGrad 算法

在动量法中，当 x1 和 x2 的梯度值有较⼤差别时，需要选择⾜够小的学习率使得⾃变量在梯度值较⼤的维度上不发散。但这样会导致⾃变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得⾃变量的更新⽅向更加⼀致，从而降低发散的可能。 而 AdaGrad 算法 ，它根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题  。

  - AdaGrad 算法在迭代过程中不断调整学习率，并让⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。
  - 使⽤ AdaGrad 算法时，⾃变量中每个元素的学习率在迭代过程中⼀直在降低（或不变）。 

**AdaGrad 算法的问题：**当学习率在迭代早期降得较快且当前解依然不佳时， AdaGrad算法在迭代后期
由于学习率过小，可能较难找到⼀个有⽤的解。 

### RMSProp 算法 

RMSProp 算法是为了解决 AdaGrad 算法而提出来的

RMSProp 算法和 AdaGrad 算法的不同在于， RMSProp 算法使⽤了小批量随机梯度按元素平⽅的指数加权移动平均来调整学习率。 **如此一来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。** 

### AdaDelta 算法 

也是对 AdaGrad 算法的一种改进，但是与 RMSProp 不同的是 AdaDelta 算法没有学习率这⼀超参数。 

- AdaDelta 算法没有学习率超参数，它通过使⽤有关⾃变量更新量平⽅的指数加权移动平均的项来替代 RMSProp 算法中的学习率。 

### Adam 算法 

Adam 算法在 RMSProp 算法基础上对小批量随机梯度也做了指数加权移动平均 

- Adam算法在 RMSProp 算法的基础上对小批量随机梯度也做了指数加权移动平均。
- Adam算法使⽤了偏差修正。 