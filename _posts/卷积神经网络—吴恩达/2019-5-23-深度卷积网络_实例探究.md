---
layout: post
title: deeplearning_ai 深度卷积网络—实例探究
date: 2019-5-23
tag: 卷积神经网络
---


[TOC]

### 经典卷积网络

1. LeNet

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g37v4izsefj210s0kegoe.jpg)

2. AlexNet

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g37v5xe67cj21190ketc0.jpg)

3. VGG-16（16 指的是该网络架构中卷积层和全连接层的个数总和）

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g37v74ew5oj21120kljuy.jpg)

三篇论文的阅读顺序：2-3-1

### ResNets

深的网络，会导致**梯度消失**和**梯度爆炸**

- Residual block（残差块）ResNets 网络是由许多残差块构成的

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g37vixk68zj210m0kctba.jpg)

- 有了残差神经网络使得训练更深的网络成为可能，训练的错误也会降低

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g37vku3z9fj210m0k9gpb.jpg)

### 1×1 卷积的作用

**压缩信道数量并减少计算**

类似全连接层

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g3b0rwisjjj20ms0fggps.jpg)

### Inception 网络

使用开源项目加上开源的框架来进行开发

下载：`git clone url` 

查看文件更多信息：`more filename`

### 迁移学习

用别人训练好的网络或者权重来实现自己的项目

- 数据集较少时：冻结预训练网络的大部分层，只训练最后几层，在训练时，先将数据经过冻结的几层得到输出，存放到硬盘中，可以减少后面几层训练的成本
- 数据集数据中等时：冻结预训练网络的前几层
- 大量数据集：使用别人的网络框架，但是不使用别人训练好的权重，根据自己的实际情况训练整个网络

### 数据增强

- 常使用的数据增强方法：镜像、随机裁剪
- 其他方法：rotation、shearing、local warping、色彩转换
- AlexNet 文献中介绍了 PCA（称作：颜色增强）
- 实际操作：使用多线程或者多进程加载数据、实现并行运算

**还需要学习的：看看别人的文献是如何实现数据增强的，采用了哪些方法实现数据增强，为什么，针对不同的项目如何进行选择。**

**学习多线程多进程读取数据的方法，并用代码实现，用在以后的项目中。**

#### 计算机视觉的现状及一些建议

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g3b2ujvyjuj20ny07zq5y.jpg)

机器学习中知识的来源：

- 获取的数据
- 手工工程
  - 手工设计工程特性
  - 网络的架构
  - 其他

在竞赛或者实际工程运用的一些建议：

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g3b2x5e9owj20xq0irtma.jpg)

- 使用集成：训练几个不同的网络，在实际运用时对一个测试数据进行预测，结果取所有网络的平均值作为最后的结果，会占据大量的内存
- crop：不会占据大量的内存，但是运行速度还是很慢

以上的两个 tips 在实际工程部署上不是很推荐，但也不一定，得综合进行考虑，结合成本、计算机资源等。