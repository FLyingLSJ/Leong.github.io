[TOC]



卷积神经网络的使用情形：

### Padding（填充）

1. 为什么需要进行 Padding 操作

- 不使用 Padding  会使每次进行卷积运算后图像缩小
- 不使用 Padding  图像的边缘信息会丢失

2. 常用的 Padding 操作

- Valid：不填充 **<u>n×n * f×f => (n-f+1)×(n-f+1)</u>**
- Same：填充 **<u>n×n * f×f => (n+2p-f+1)×(n+2p-f+1)</u>**  输入与输出图像的尺寸大小相同

3. f：卷积内核常常使用奇数的原因

- 内核是奇数，能使填充对称
- 内核有一个中心

### Stride

包含 padding 和 stride 操作的图像变换公式：n×n * f×f => $(\frac{n+2 p-f}{s}+1) ×(\frac{n+2 p-f}{s}+1)$

其中：padding：p  、stride：s ，若输出结果不是整数，这需要做向下取整

几个名词：互相关和卷积，实际上互相关是就是以上的图像操作，卷积操作的实际上在元素乘积求和之前要对内核进行一个沿水平轴和竖直做翻转的步骤，但是在图像处理上，卷积操作不需要这一步，我们一般称卷积操作而不叫互相关。

### 立体卷积

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g36p1pff05j20ut0cegok.jpg)

![](https://ws1.sinaimg.cn/large/acbcfa39ly1g36pe6lkpyj210g0hiq4s.jpg)

对立体图像进行卷积的公式：其中，要求图片的通道数和卷积的内核通道数需要一样，$n_{c}^{\prime}$ 等于内核的个数，每个内核代表提取不同的特征

$n×n×n_c$ * $f×f×n_c$ = $(n-f+1) × (n-f+1) \times n_{c}^{\prime}$ 

### 单层卷积神经网络

卷积神经网络的推导

![](https://ws1.sinaimg.cn/large/acbcfa39gy1g36pzw3rhwj20xr0hkn8n.jpg)

例子：127×127×3 * 3×3×3 -> 125×125×64

### 一些概念

卷积层（Conv）

池化层（Pool）：

1. 池化层的作用
   - 缩减模型的规模
   - 提高计算速度
   - 提高所提取特征的鲁棒性
2. 超参数设置
   - 常用超参数：stride=2 滤波器大小 f=2 （此参数下高宽缩减一半，池化层很少使用 padding）
   - 常使用最大池化层（Max pooling）
3. 其他
   - 池化层不会改变通道数，对每个通道单独执行
   - 池化层没有要学习的参数

![](https://ws1.sinaimg.cn/large/acbcfa39ly1g36roh1irej20zd0hl76e.jpg)

全连接层（FC）

### 卷积神经网络的优势

1. 参数共享
2. 稀疏连接：网络的输出只与输入的部分数据有关

以上两个机制，使得卷积神经网络比全连接网络更具优势，能够减少参数和减少过拟合

多看看文献，看看别人的框架并用到自己的应用中